{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "032ecd0c",
   "metadata": {},
   "source": [
    "# CRNN Model for Heart Murmur Detection\n",
    "## Deep Learning Pipeline with Stabilizer Method\n",
    "\n",
    "This notebook implements a hybrid CRNN (Convolutional Recurrent Neural Network) architecture to detect heart murmurs with improved sensitivity by capturing both spatial (sound) and temporal (rhythm) features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcdce36",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc956e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import os\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc85b66",
   "metadata": {},
   "source": [
    "## 2. Configuration and Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a476be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SAMPLE_RATE = 1000  # Hz\n",
    "DURATION = 5  # seconds\n",
    "TARGET_LENGTH = 5000  # samples (SAMPLE_RATE * DURATION)\n",
    "LOWCUT = 20  # Hz\n",
    "HIGHCUT = 400  # Hz\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = r'/pcg_data2/training_data/training_data'\n",
    "CSV_PATH = r'/pcg_data2/training_data.csv'\n",
    "\n",
    "print(f\"Data Directory: {DATA_DIR}\")\n",
    "print(f\"CSV Path: {CSV_PATH}\")\n",
    "print(f\"Directory exists: {os.path.exists(DATA_DIR)}\")\n",
    "print(f\"CSV exists: {os.path.exists(CSV_PATH)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa4506",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e026965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV with patient-level labels\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Total patients in CSV: {len(df)}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nMurmur distribution:\")\n",
    "print(df['Murmur'].value_counts())\n",
    "\n",
    "# Filter out 'Unknown' labels (we only want binary classification)\n",
    "df = df[df['Murmur'] != 'Unknown'].copy()\n",
    "print(f\"\\nAfter removing Unknown: {len(df)} patients\")\n",
    "\n",
    "# Create binary labels: Present=1 (Abnormal), Absent=0 (Normal)\n",
    "df['Label'] = (df['Murmur'] == 'Present').astype(int)\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['Label'].value_counts())\n",
    "print(f\"Class balance: {df['Label'].value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dfc241",
   "metadata": {},
   "source": [
    "## 4. Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfec4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    \"\"\"Apply bandpass filter to remove noise\"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    filtered_data = filtfilt(b, a, data)\n",
    "    return filtered_data\n",
    "\n",
    "def preprocess_audio(file_path, target_length=TARGET_LENGTH, sr=SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Load and preprocess audio file:\n",
    "    1. Load with librosa\n",
    "    2. Apply bandpass filter (20-400Hz)\n",
    "    3. Crop or pad to target length\n",
    "    4. Max-Abs normalization\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio\n",
    "        audio, _ = librosa.load(file_path, sr=sr, mono=True)\n",
    "        \n",
    "        # Apply bandpass filter\n",
    "        audio = butter_bandpass_filter(audio, LOWCUT, HIGHCUT, sr)\n",
    "        \n",
    "        # Crop or pad to target length\n",
    "        if len(audio) > target_length:\n",
    "            # Crop from center\n",
    "            start = (len(audio) - target_length) // 2\n",
    "            audio = audio[start:start + target_length]\n",
    "        elif len(audio) < target_length:\n",
    "            # Pad with zeros\n",
    "            padding = target_length - len(audio)\n",
    "            audio = np.pad(audio, (0, padding), mode='constant')\n",
    "        \n",
    "        # Max-Abs Normalization\n",
    "        max_val = np.max(np.abs(audio))\n",
    "        if max_val > 0:\n",
    "            audio = audio / max_val\n",
    "        \n",
    "        return audio\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test preprocessing on a sample file\n",
    "sample_files = [f for f in os.listdir(DATA_DIR) if f.endswith('.wav')][:1]\n",
    "if sample_files:\n",
    "    sample_path = os.path.join(DATA_DIR, sample_files[0])\n",
    "    sample_audio = preprocess_audio(sample_path)\n",
    "    print(f\"Sample audio shape: {sample_audio.shape}\")\n",
    "    print(f\"Sample audio range: [{sample_audio.min():.4f}, {sample_audio.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3eca4",
   "metadata": {},
   "source": [
    "## 5. The \"Stabilizer\" Method: Manual Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112d9cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create file list with labels\n",
    "file_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    patient_id = str(row['Patient ID'])\n",
    "    label = row['Label']\n",
    "    \n",
    "    # Find all .wav files for this patient\n",
    "    matching_files = [f for f in os.listdir(DATA_DIR) if f.startswith(patient_id) and f.endswith('.wav')]\n",
    "    \n",
    "    for wav_file in matching_files:\n",
    "        file_path = os.path.join(DATA_DIR, wav_file)\n",
    "        file_list.append({'file_path': file_path, 'label': label})\n",
    "\n",
    "print(f\"Total audio files found: {len(file_list)}\")\n",
    "\n",
    "# Count labels before balancing\n",
    "label_counts = Counter([item['label'] for item in file_list])\n",
    "print(f\"\\nLabel distribution before balancing:\")\n",
    "print(f\"Normal (0): {label_counts[0]}\")\n",
    "print(f\"Abnormal (1): {label_counts[1]}\")\n",
    "\n",
    "# **Manual Random Oversampling** (The Stabilizer)\n",
    "normal_files = [item for item in file_list if item['label'] == 0]\n",
    "abnormal_files = [item for item in file_list if item['label'] == 1]\n",
    "\n",
    "# Determine target count (max of the two classes)\n",
    "max_count = max(len(normal_files), len(abnormal_files))\n",
    "\n",
    "print(f\"\\nTarget count per class: {max_count}\")\n",
    "\n",
    "# Oversample the minority class\n",
    "if len(abnormal_files) < max_count:\n",
    "    # Calculate how many times to duplicate\n",
    "    duplicates_needed = max_count - len(abnormal_files)\n",
    "    oversampled = random.choices(abnormal_files, k=duplicates_needed)\n",
    "    abnormal_files.extend(oversampled)\n",
    "    print(f\"Oversampled Abnormal files: {len(abnormal_files)}\")\n",
    "elif len(normal_files) < max_count:\n",
    "    duplicates_needed = max_count - len(normal_files)\n",
    "    oversampled = random.choices(normal_files, k=duplicates_needed)\n",
    "    normal_files.extend(oversampled)\n",
    "    print(f\"Oversampled Normal files: {len(normal_files)}\")\n",
    "\n",
    "# Combine and shuffle\n",
    "balanced_file_list = normal_files + abnormal_files\n",
    "random.shuffle(balanced_file_list)\n",
    "\n",
    "print(f\"\\nTotal files after balancing: {len(balanced_file_list)}\")\n",
    "balanced_label_counts = Counter([item['label'] for item in balanced_file_list])\n",
    "print(f\"Normal (0): {balanced_label_counts[0]}\")\n",
    "print(f\"Abnormal (1): {balanced_label_counts[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa11429",
   "metadata": {},
   "source": [
    "## 6. Load and Preprocess All Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368b27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all audio data\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "print(\"Loading and preprocessing audio files...\")\n",
    "for i, item in enumerate(balanced_file_list):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Processed {i + 1}/{len(balanced_file_list)} files...\")\n",
    "    \n",
    "    audio = preprocess_audio(item['file_path'])\n",
    "    if audio is not None:\n",
    "        X_data.append(audio)\n",
    "        y_data.append(item['label'])\n",
    "\n",
    "X_data = np.array(X_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "print(f\"\\nData loaded successfully!\")\n",
    "print(f\"X shape: {X_data.shape}\")\n",
    "print(f\"y shape: {y_data.shape}\")\n",
    "print(f\"Label distribution: {Counter(y_data)}\")\n",
    "\n",
    "# Reshape for Conv1D: (samples, timesteps, features)\n",
    "X_data = X_data.reshape(-1, TARGET_LENGTH, 1)\n",
    "print(f\"X reshaped: {X_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d595a",
   "metadata": {},
   "source": [
    "## 7. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
    "print(f\"\\nTrain label distribution: {Counter(y_train)}\")\n",
    "print(f\"Test label distribution: {Counter(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03197f10",
   "metadata": {},
   "source": [
    "## 8. Build CRNN Architecture (The Hybrid Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7829c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_crnn_model(input_shape=(TARGET_LENGTH, 1)):\n",
    "    \"\"\"\n",
    "    CRNN Architecture:\n",
    "    - Block 1 (The Ear): Conv1D + BatchNorm + MaxPooling\n",
    "    - Block 2: Conv1D + BatchNorm + MaxPooling\n",
    "    - Block 3 (The Brain): Bidirectional GRU + Dropout\n",
    "    - Output: Dense (sigmoid)\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Block 1: The Ear - Captures spatial sound features\n",
    "        layers.Conv1D(filters=32, kernel_size=10, activation='relu', name='conv1'),\n",
    "        layers.BatchNormalization(name='bn1'),\n",
    "        layers.MaxPooling1D(pool_size=4, name='pool1'),  # Aggressive pooling\n",
    "        \n",
    "        # Block 2: Deeper sound feature extraction\n",
    "        layers.Conv1D(filters=64, kernel_size=5, activation='relu', name='conv2'),\n",
    "        layers.BatchNormalization(name='bn2'),\n",
    "        layers.MaxPooling1D(pool_size=4, name='pool2'),\n",
    "        \n",
    "        # Block 3: The Brain - Learns temporal rhythm patterns\n",
    "        layers.Bidirectional(\n",
    "            layers.GRU(units=64, return_sequences=False),\n",
    "            name='bi_gru'\n",
    "        ),\n",
    "        layers.Dropout(0.4, name='dropout'),  # Prevent memorization\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(1, activation='sigmoid', name='output')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_crnn_model()\n",
    "model.summary()\n",
    "\n",
    "# Calculate output shape after convolutions\n",
    "print(\"\\n--- Shape Transformations ---\")\n",
    "print(f\"Input: (batch, {TARGET_LENGTH}, 1)\")\n",
    "print(f\"After Conv1D(32, 10) + Pool(4): (batch, {TARGET_LENGTH // 4}, 32)\")\n",
    "print(f\"After Conv1D(64, 5) + Pool(4): (batch, {TARGET_LENGTH // 16}, 64)\")\n",
    "print(f\"After Bidirectional GRU(64): (batch, 128)\")\n",
    "print(f\"After Dense(1): (batch, 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b964980",
   "metadata": {},
   "source": [
    "## 9. Compile Model with Safety Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a66c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer with gradient clipping (MANDATORY for stability)\n",
    "optimizer = Adam(learning_rate=0.0001, clipnorm=1.0)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', \n",
    "             keras.metrics.Precision(name='precision'),\n",
    "             keras.metrics.Recall(name='recall')]\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(f\"Optimizer: Adam (lr=0.0001, clipnorm=1.0)\")\n",
    "print(f\"Loss: binary_crossentropy\")\n",
    "print(f\"Metrics: accuracy, precision, recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd20141",
   "metadata": {},
   "source": [
    "## 10. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d763f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5108d2",
   "metadata": {},
   "source": [
    "## 11. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f253dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy and Loss curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "axes[0].plot(history.history['accuracy'], label='Train Accuracy', marker='o')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', marker='s')\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "axes[1].plot(history.history['loss'], label='Train Loss', marker='o')\n",
    "axes[1].plot(history.history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pcg_training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {history.history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c00387",
   "metadata": {},
   "source": [
    "## 12. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc4d06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"TEST SET EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Loss: {test_loss:.4f}\")\n",
    "print(f\"Accuracy: {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1-Score: {2 * (test_precision * test_recall) / (test_precision + test_recall):.4f}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfac1ea",
   "metadata": {},
   "source": [
    "## 13. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214b8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nCLASSIFICATION REPORT\")\n",
    "print(\"=\" * 60)\n",
    "report = classification_report(\n",
    "    y_test, y_pred, \n",
    "    target_names=['Normal (0)', 'Abnormal (1)'],\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Additional metrics\n",
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef\n",
    "\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7624a",
   "metadata": {},
   "source": [
    "## 14. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f2d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=['Normal', 'Abnormal'],\n",
    "    yticklabels=['Normal', 'Abnormal'],\n",
    "    cbar_kws={'label': 'Count'}\n",
    ")\n",
    "plt.title('Confusion Matrix (Threshold = 0.5)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('pcg_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate percentages\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "print(\"\\nConfusion Matrix (Counts):\")\n",
    "print(cm)\n",
    "print(\"\\nConfusion Matrix (Percentages):\")\n",
    "print(cm_percent)\n",
    "\n",
    "# Calculate sensitivity and specificity\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "print(f\"\\nSensitivity (Recall): {sensitivity:.4f} ({sensitivity*100:.2f}%)\")\n",
    "print(f\"Specificity: {specificity:.4f} ({specificity*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff9dceb",
   "metadata": {},
   "source": [
    "## 15. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "MODEL_SAVE_PATH = r'\\heart_sound_models\\pcg_crnn_model.keras'\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(MODEL_SAVE_PATH), exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f\"Model saved to: {MODEL_SAVE_PATH}\")\n",
    "\n",
    "# Save training history\n",
    "history_path = r'/heart_sound_models/pcg_training_history.npy'\n",
    "np.save(history_path, history.history)\n",
    "print(f\"Training history saved to: {history_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f3958",
   "metadata": {},
   "source": [
    "## 16. Summary and Key Improvements\n",
    "\n",
    "### Key Features of this CRNN Implementation:\n",
    "\n",
    "1. **The Stabilizer Method**: Manual random oversampling ensures balanced training\n",
    "2. **Hybrid Architecture**: \n",
    "   - Conv1D layers capture spatial sound patterns (The Ear)\n",
    "   - Bidirectional GRU learns temporal rhythm patterns (The Brain)\n",
    "3. **Safety Configuration**:\n",
    "   - Gradient clipping (clipnorm=1.0) prevents exploding loss\n",
    "   - Low learning rate (1e-4) ensures stable training\n",
    "   - Dropout (0.4) prevents overfitting\n",
    "4. **Preprocessing Pipeline**:\n",
    "   - Bandpass filter (20-400Hz) removes noise\n",
    "   - Fixed duration (5s) for consistent input\n",
    "   - Max-Abs normalization for stable gradients\n",
    "\n",
    "### Expected Improvements:\n",
    "- **Higher Sensitivity**: Bidirectional GRU captures forward and backward temporal dependencies\n",
    "- **Better Generalization**: Balanced data and dropout prevent overfitting\n",
    "- **Stable Training**: Gradient clipping and low learning rate prevent loss spikes"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
